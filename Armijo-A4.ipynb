{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Solution to the Towers of Hanoi Puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damian Armijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import neuralnetworks as nnQ\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statistics import mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "This jupyterNotebook begins by displaying and describing the functions used to get the Qlearning algorithm to be used for the Towers of Hanoi problem. It then tests each of these functions to show that they are working. After this there is a section on the investigation of what inputs are best for this Q learning algortihm. Finally there is a section discussing the results found in the investigation section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "The following functions are used to implement the Q learning algorithm applied to the Towers of Hanoi problem.\n",
    "Each function should have a brief description on how what it does and how it relates to the Q learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is a helper function, which turns state and move into tuples, this is necessary for iterating over them in the Dictionary Q, it will return the \"tuplefide\" state and move pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stateMoveTuple(state, move):\n",
    "    tempTupState = []\n",
    "    for x in state:\n",
    "        tempTupState.append(tuple(x))        \n",
    "    return (tuple(tempTupState),tuple(move))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validMoves function takes in a state checks to see what moves can be made from the current state, it checks the tops of each of the columns and sees if this top can move to either of the other two columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validMoves(state):\n",
    "    validMoves = []\n",
    "    for x in range(1,4):\n",
    "        for y in range(1,4):\n",
    "            if len(state[x-1]) > 0:\n",
    "                topx = state[x-1]\n",
    "                if len(state[y-1]) !=0:\n",
    "                    topy = state[y-1]\n",
    "                if len(state[y-1]) == 0:\n",
    "                    validMoves.append([x,y])\n",
    "                elif((topx != topy and topy > topx)):\n",
    "                    validMoves.append([x,y])\n",
    "                elif((topx != topy and topy > topx)):\n",
    "                    validMoves.append([x,y])\n",
    "                elif((topx != topy and topy > topx)):\n",
    "                    validMoves.append([x,y])\n",
    "\n",
    "    return validMoves                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The printState function simply prints out the state given to it in a format that is nice to read. It gives a visual of the columns and the \"rings\" on them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printState(state):\n",
    "    temp = 0\n",
    "    col1Len = 0\n",
    "    col2Len = 0\n",
    "    col3Len = 0\n",
    "    big = len(max(state))\n",
    "    while temp < len(max(state)):\n",
    "        holder1,holder2,holder3 = \" \",\" \",\" \"\n",
    "        \n",
    "        if col1Len< len(state[0]) and col1Len !=big and col1Len == temp:\n",
    "            holder1 = state[0][temp]\n",
    "            col1Len = col1Len + 1\n",
    "        if col2Len < len(state[1]) and col2Len == temp:\n",
    "            holder2 = state[1][temp]\n",
    "            col2Len = col2Len + 1\n",
    "        if col3Len < len(state[2])and col3Len == temp:\n",
    "            holder3 = state[2][temp]\n",
    "            col3Len = col3Len + 1\n",
    "            \n",
    "        print(holder1,holder2,holder3)\n",
    "        temp = temp+1\n",
    "    print(\"------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The makeMove function returns a copy of the given state after it has taken the move which is given to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeMove(state, move):\n",
    "    state2 = deepcopy(state)\n",
    "    temp = state2[move[0]-1][0]\n",
    "    state2[move[0]-1].pop(0)\n",
    "    state2[move[1]-1].insert(0,temp)\n",
    "    return state2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trainQ function is what actually creates and trains the Q dictionary (the State,Action dictionary). It goes through the given amount of reputations, and updates the value of the Q dictionary based on the choice decided from the epsilonFindMoves(greedyEpsilon). It also decays the epsilon given to the epsilonFindMoves function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainQ(nRepetitions, learningRate, epsilonDecayFactor, validMovesF, makeMoveF):\n",
    "    Q = {}\n",
    "    \n",
    "    state = [[1,2,3],[],[]]\n",
    "    epsilon = 1\n",
    "    step_count = []\n",
    "    \n",
    "    for x in range(nRepetitions):\n",
    "        \n",
    "        epsilon = epsilonDecayFactor * epsilon\n",
    "        step = 0\n",
    "        goal_state = [[],[],[1,2,3]]\n",
    "        done = False\n",
    "        \n",
    "        while done != True:\n",
    "            \n",
    "            step = step + 1\n",
    "            next_move = epsilonFindMoves(Q, state, epsilon, validMovesF)\n",
    "            #print(state,next_move)\n",
    "            next_state = makeMove(state,next_move) \n",
    "            \n",
    "            if stateMoveTuple(state,next_move) not in Q:\n",
    "                Q[stateMoveTuple(state,next_move)] = 0\n",
    "                \n",
    "            if next_state == goal_state:\n",
    "                Q[stateMoveTuple(state,next_move)] = 1\n",
    "                done = True\n",
    "                \n",
    "            if step > 1:\n",
    "                Q[stateMoveTuple(stateOld,moveOld)] += learningRate *(1+Q[stateMoveTuple(state,next_move)]-\n",
    "                                                        Q[stateMoveTuple(stateOld,moveOld)])\n",
    "            stateOld, moveOld = state, next_move\n",
    "            state = next_state\n",
    "        state = [[1,2,3],[],[]]\n",
    "        step_count.append(step)\n",
    "    return Q ,step_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testQ function takes in the Q dictionary that was trained, and picks the most greedy option towards the goal. It takes each move and puts it in a list, this list is then returned when the goal is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testQ(Q, maxSteps, validMovesF, makeMovesF):\n",
    "    \n",
    "    state = [[1,2,3],[],[]]\n",
    "    epsilon = 0\n",
    "    path = []\n",
    "    #path.append(state)\n",
    "    \n",
    "    step = 0\n",
    "    goal_state = [[],[],[1,2,3]]\n",
    "    done = False\n",
    "\n",
    "    while done != True:\n",
    "        step = step+1\n",
    "        path.append(state)\n",
    "        next_move = epsilonFindMoves(Q, state, epsilon, validMovesF)\n",
    "\n",
    "        next_state = makeMove(state,next_move) \n",
    "        \n",
    "\n",
    "        if next_state == goal_state:\n",
    "            done = True\n",
    "        if step > maxSteps:\n",
    "            done = True\n",
    "        state = next_state\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epsilonFindMoves(...) function takes in the Q dictionary and finds which of the valid moves from state is most likely the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilonFindMoves(Q, state, epsilonRate, validMovesF):\n",
    "    validMoveList = validMoves(state)\n",
    "    if np.random.uniform()<epsilonRate:\n",
    "        return validMoveList[np.random.choice(len(validMoveList))] \n",
    "    else:\n",
    "        Qs = np.array([Q.get(stateMoveTuple(state, m), 0) for m in validMoveList])\n",
    "        return validMoveList[np.argmin(Qs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    \n",
      "2    \n",
      "3    \n",
      "------\n",
      "2   1\n",
      "3    \n",
      "------\n",
      "3 2 1\n",
      "------\n",
      "3 1  \n",
      "------\n",
      "  1 3\n",
      "------\n",
      "1 2 3\n",
      "------\n",
      "1   2\n",
      "    3\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for x in path:\n",
    "    printState(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [[1,2,3],[],[]]\n",
    "move = [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((1, 2, 3), (), ()), (1, 2))\n"
     ]
    }
   ],
   "source": [
    "#testing stateMoveTuple(state,move)\n",
    "print(stateMoveTuple(state,move))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [1, 3]]\n"
     ]
    }
   ],
   "source": [
    "#testing validMoves(state)\n",
    "print(validMoves(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    \n",
      "2    \n",
      "3    \n",
      "------\n"
     ]
    }
   ],
   "source": [
    "#testing printState(state)\n",
    "printState(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [1], []]\n"
     ]
    }
   ],
   "source": [
    "#testing makeMove(state,move)\n",
    "print(makeMove(state,move))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Actions in Q: 76\n",
      "Progression of steps to goal:\n",
      "[ 95  37 237  19  23  45  26  23   7  31  14  44   8  26  10   9   8  14\n",
      "   7  32  11   7  16   7   7   7   7   7   9   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7]\n",
      "Mean of steps: 7.578\n"
     ]
    }
   ],
   "source": [
    "#testing trainQ(...)\n",
    "Q, stepsToGoal = trainQ(50, 0.5, 0.7, validMoves, makeMove)\n",
    "print('State Actions in Q:' ,len(Q))\n",
    "print(\"Progression of steps to goal:\")\n",
    "print(np.array(stepsToGoal))\n",
    "print('Mean of steps:',mean(steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path for trained Q to Goal:\n",
      "1    \n",
      "2    \n",
      "3    \n",
      "------\n",
      "2   1\n",
      "3    \n",
      "------\n",
      "3 2 1\n",
      "------\n",
      "3 1  \n",
      "------\n",
      "  1 3\n",
      "------\n",
      "1 2 3\n",
      "------\n",
      "1   2\n",
      "    3\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "#testing testQ(...)\n",
    "Q, stepsToGoal = trainQ(100, 0.5, 0.7, validMoves, makeMove)\n",
    "path = testQ(Q, 20, validMoves, makeMove)\n",
    "print(\"Path for trained Q to Goal:\")\n",
    "for s in path:\n",
    "    printState(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 2, 3], [], []],\n",
       " [[2, 3], [], [1]],\n",
       " [[3], [2], [1]],\n",
       " [[3], [1, 2], []],\n",
       " [[], [1, 2], [3]],\n",
       " [[1], [2], [3]],\n",
       " [[1], [], [2, 3]]]"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Repetitions, .5 learning rate, .7 decay rate\n",
      "State Actions in Q: 76\n",
      "Progression of steps to goal:\n",
      "[107  36 198  12  35  18  41  61  24  55   9  17  24   9   9   9  10  36\n",
      "  38   7   7  18   7   7  11   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7]\n",
      "Mean of steps: 8.26\n"
     ]
    }
   ],
   "source": [
    "Q, stepsToGoal = trainQ(500, 0.5, 0.7, validMoves, makeMove)\n",
    "print('500 Repetitions, .5 learning rate, .7 decay rate')\n",
    "print('State Actions in Q:' ,len(Q))\n",
    "print(\"Progression of steps to goal:\")\n",
    "print(np.array(stepsToGoal[:100]))\n",
    "print('Mean of steps:',mean(stepsToGoal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repetitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Repetitions\n",
      "State Actions in Q: 76\n",
      "Progression of steps to goal:\n",
      "[ 87 119  51  44  15]\n",
      "Mean of steps: 63.2\n"
     ]
    }
   ],
   "source": [
    "Q, stepsToGoal = trainQ(5, 0.5, 0.7, validMoves, makeMove)\n",
    "print('5 Repetitions')\n",
    "print('State Actions in Q:' ,len(Q))\n",
    "print(\"Progression of steps to goal:\")\n",
    "print(np.array(stepsToGoal))\n",
    "print('Mean of steps:',mean(stepsToGoal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Repetitions\n",
      "State Actions in Q: 76\n",
      "Progression of steps to goal:\n",
      "[ 51  74 108  55  23  50  15  24  50  18  54  10  16   8  27   7  28  33\n",
      "   9  12   7   7   9  24  11  23   7   7   7   7   8   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7]\n",
      "Mean of steps: 8.144\n"
     ]
    }
   ],
   "source": [
    "Q, stepsToGoal = trainQ(500, 0.5, 0.7, validMoves, makeMove)\n",
    "print('500 Repetitions')\n",
    "print('State Actions in Q:' ,len(Q))\n",
    "print(\"Progression of steps to goal:\")\n",
    "print(np.array(stepsToGoal[:100]))\n",
    "print('Mean of steps:',mean(stepsToGoal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Repetitions\n",
      "Progression of steps to goal:\n",
      "[118  47 118  52  16  34  16  45  36  58  10   7  26   9  15  30  12  10\n",
      "  15   9  15   7   7   8   7   7  21   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7]\n",
      "Mean of steps: 7.0566\n"
     ]
    }
   ],
   "source": [
    "Q, stepsToGoal = trainQ(10000, 0.5, 0.7, validMoves, makeMove)\n",
    "print('10000 Repetitions')\n",
    "print(\"Progression of steps to goal:\")\n",
    "print(np.array(stepsToGoal[:100]))\n",
    "print('Mean of steps:',mean(stepsToGoal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Repetitions, .99 learning rate\n",
      "Progression of steps to goal:\n",
      "[ 24  91 125  21  37  37  22  13  16   7  11   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7]\n",
      "Mean of steps: 7.654\n",
      "500 Repetitions, .01 learning rate\n",
      "Progression of steps to goal:\n",
      "[  28  208   90  751  407  561  325  238  903  756  137 3281  188  108\n",
      "   52  125   65   79  123   75  107   57  167   27   97  104   49  113\n",
      "   24  118   49   90   50  116   69   63  147   22   80   55   78   46\n",
      "   86  124   27  121   13   70   70   57   62   45   88   50   71  118\n",
      "   16   73   84   65   42   81   35   46  118   21  122   38   35   46\n",
      "   86   22   32  106   18   78   59   40   46   63   62   47   93   30\n",
      "   41   69   45   38   90   58   17   81   27   34   56  105   31   40\n",
      "   68   39]\n",
      "Mean of steps: 51.302\n"
     ]
    }
   ],
   "source": [
    "Q, stepsToGoal = trainQ(500, .99 , 0.7, validMoves, makeMove)\n",
    "print('500 Repetitions, .99 learning rate')\n",
    "print(\"Progression of steps to goal:\")\n",
    "print(np.array(stepsToGoal[:100]))\n",
    "print('Mean of steps:',mean(stepsToGoal))\n",
    "Q, stepsToGoal = trainQ(500, .01 , 0.7, validMoves, makeMove)\n",
    "print('500 Repetitions, .01 learning rate')\n",
    "print(\"Progression of steps to goal:\")\n",
    "print(np.array(stepsToGoal[:100]))\n",
    "print('Mean of steps:',mean(stepsToGoal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon decay rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Repetitions, .99 decay\n",
      "Progression of steps to goal:\n",
      "[ 99 137  68  92 209 253 243 203  66  34  36 105 278  23  53  26  67  36\n",
      "  81  26  70  54  10  58  26  21  46  96  12  31  49  18  40  19  15  55\n",
      "  21  41  15  13  29  24  34  23  27  20  23  15   9  28  16  44  11  15\n",
      "  11  14   8  33   9  13  11  19  39  16  19  12  17  22  11  36  18   9\n",
      "  23  12  10  20  13  27  21  16  29  24  24  21  13  11  13  30  16  10\n",
      "  16  10  28  10   8  14  18  22   8  12]\n",
      "Mean of steps: 14.328\n",
      "500 Repetitions, .01 decay\n",
      "Progression of steps to goal:\n",
      "[ 66  42  86 116  26  26  21  40  27  29  12  22  45  16  15  11  10  36\n",
      "  19   7   7  10  10  42  10   8   7   7   7   7   7  10   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7   7\n",
      "   7   7   7   7   7   7   7   7   7   7]\n",
      "Mean of steps: 8.16\n"
     ]
    }
   ],
   "source": [
    "Q, stepsToGoal = trainQ(500, 0.5, 0.99, validMoves, makeMove)\n",
    "print('500 Repetitions, .99 decay')\n",
    "print(\"Progression of steps to goal:\")\n",
    "print(np.array(stepsToGoal[:100]))\n",
    "print('Mean of steps:',mean(stepsToGoal))\n",
    "Q, stepsToGoal = trainQ(500, 0.5, 0.01, validMoves, makeMove)\n",
    "print('500 Repetitions, .01 decay')\n",
    "print(\"Progression of steps to goal:\")\n",
    "print(np.array(stepsToGoal[:100]))\n",
    "print('Mean of steps:',mean(stepsToGoal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best of 3 options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Repetitions, .99 learning rate, .01 decay\n",
      "Progression of steps to goal:\n",
      "[66 42 62 36 80 47 11 15 10 24  7 14  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7  7\n",
      "  7  7  7  7]\n",
      "Mean of steps: 7.033\n"
     ]
    }
   ],
   "source": [
    "Q, stepsToGoal = trainQ(10000, 0.99, 0.01, validMoves, makeMove)\n",
    "print('10000 Repetitions, .99 learning rate, .01 decay')\n",
    "print(\"Progression of steps to goal:\")\n",
    "print(np.array(stepsToGoal[:100]))\n",
    "print('Mean of steps:',mean(stepsToGoal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were some very interesting things that I found when testing the trainQ function with different inputs. When testing with different repetitions it was very clear to see that there is a clear benefit to doing more repetitions. Doing only 5 repetitions meant that it took many more than 7 steps to get to the goal, and doing 500 repetitions always reached 7 steps. It didn't seem however that doing an excessive amount of repetitions made it quicker to get to 7 steps. \n",
    "\n",
    "I also tested with differing learning rates both .99 and .01. This did seem to make a big impact on the \"learning\". The .99 learning rate was able to very quickly lower it down to only 7 moves, it was faster than the control. This was completely the opposite of the .01 learning rate, which only a couple times randomly got 7 moves.\n",
    "\n",
    "I then tested different epsilon decay rates .99 and .01. This did seem to make an impact the decay rate of .01 did reach doing 7 moves at about the same rate as the control, whereas .99 took a while to reach 8 moves. Changing decay rate didn't seem to have as much a positive impact as it would have a negative one when compared to the control.\n",
    "\n",
    "After doing this, I took the best input option from all the different testings, and it did significantly better than the control. The control had an average of 8.26 and took about 25 repetitions to reach a constant of 7 moves. This is much less effective when compared to the best of all the options I tested, that one had an average of 7.033 moves, and took only 12 to reach a constant of 7 moves. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
